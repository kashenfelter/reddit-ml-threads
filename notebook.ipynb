{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_agent = \"TickFaw 0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = praw.Reddit(user_agent = user_agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r.config.store_json_result = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subreddit = r.get_subreddit(\"machinelearning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 :: [Discussion] Join us on /r/LearnMachineLearning!\n",
      "9 :: [D] Machine Learning - WAYR (What Are You Reading) - Week 13\n"
     ]
    }
   ],
   "source": [
    "for post in subreddit.get_hot(limit=2):\n",
    "    print(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from praw.helpers import flatten_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_comments(_comments):\n",
    "    comments = []\n",
    "    for obj in _comments:\n",
    "        inner_comments = []\n",
    "        for obj_item in obj:\n",
    "            try:\n",
    "                inner_comments.append(obj_item.body)\n",
    "            except AttributeError:\n",
    "                pass\n",
    "        comments.append(inner_comments)\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posts = []\n",
    "for post in subreddit.get_hot(limit=3):\n",
    "    comments = []\n",
    "    _comments = [ [comment] + flatten_tree(comment.replies) for comment in post.comments ]\n",
    "    posts.append({'title' : post.title, 'comments' : extract_comments(_comments)})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Is this real?  \n",
      "_______________________________________\n",
      "Super real. \n",
      "--\n",
      "Dang.\n",
      "--\n",
      "_______________________________________\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Happy (and relieved) to see a more realistic environment without the hard-sell and hype of the previous lip reading paper. Great work! Impossible is nothing ;)\n",
      "_______________________________________\n",
      "_______________________________________\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "The paper: https://arxiv.org/abs/1611.05358\n",
      "\n",
      "So wait. Oxford and DeepMind released [this one](https://openreview.net/forum?id=BkjLkSqxg) a few weeks ago, and now Oxford and DeepMind release this paper with none of the previous authors? How do they compare \"in the wild\"? Why not collaborate?\n",
      "_______________________________________\n",
      "Here's the dataset paper to add: https://www.robots.ox.ac.uk/~vgg/publications/2016/Chung16/chung16.pdf\n",
      "\n",
      "It looks like the Oxford groups are in different departments.\n",
      "--\n",
      "...are we running out of ML problems? lol\n",
      "\n",
      "--\n",
      "Indeed. The lipnet paper came from Oxford CS department, while this one looks to be from people from the VGG, which is in the Engineering science department (where the focus is computer vision more than machine learning in some groups)\n",
      "--\n",
      "Boy this is the Google Brain/Deepmind 'concrete distribution' paper all over again.\n",
      "\n",
      "(Also, I bet everyone criticizing the previous paper for not using a natural lipreading dataset is feeling a right prat now.)\n",
      "--\n",
      "_______________________________________\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Bin blue by m four please.\n",
      "_______________________________________\n",
      "checkmate\n",
      "--\n",
      "_______________________________________\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Dumb question, but could the technique be performed 'in reverse' to produce convincing mouth movements from speech/text?\n",
      "\n",
      "edit: Anyone want to collab on it?\n",
      "_______________________________________\n",
      "I'm looking at you, Bethesda.\n",
      "--\n",
      "The fact that the images are interpreted as a shape face with dots ([p6](https://www.robots.ox.ac.uk/~vgg/publications/2016/Chung16/chung16.pdf)) to mark certain lip positions is promising to reversing it not only on a new face image, but also on a wireframe.\n",
      "--\n",
      "This combined with possible WaveNet-eque voice style transfer could potentially be used to change what people are saying in videos and edit their mouth movement to match. I bet it's not long until someone does that.\n",
      "--\n",
      "https://www.youtube.com/watch?v=ohmajJTcpNk\n",
      "--\n",
      ">[**Face2Face: Real-time Face Capture and Reenactment of RGB Videos (CVPR 2016 Oral) [6:36]**](http://youtu.be/ohmajJTcpNk)\n",
      "\n",
      ">>CVPR 2016 Paper Video (Oral)\n",
      "\n",
      "> [*^Matthias ^Niessner*](https://www.youtube.com/channel/UCXN2nYjVT0cR9G61RPEzK5Q) ^in ^Science ^& ^Technology\n",
      "\n",
      ">*^2,681,994 ^views ^since ^Mar ^2016*\n",
      "\n",
      "[^bot ^info](http://www.reddit.com/r/youtubefactsbot/wiki/index)\n",
      "--\n",
      "This year someone posted a publication that changed the gaze direction of the persons in the video, rather funny, why not on full faces?\n",
      "\n",
      "Also, face alignment methods (full 3D estimation and matching) are getting incredibly good with normal RGB cameras so yes, combining the two is not far fetched in the near future.\n",
      "--\n",
      "Any pointers to good papers/code on 3D face keypoint detection from 2D face RGB images? Thanks in advance! \n",
      "--\n",
      "I was referring to this \"famous\" [video](https://www.youtube.com/watch?v=ohmajJTcpNk) in which they map one actor's expressions onto an unsuspecting target (video). The link to the project page is in the youtube page's description.\n",
      "--\n",
      "Oh man that's actually a really cool idea I like that, I bet it totally could, what an exciting time to live in! A lot of this stuff is outside my zone of programming (I have tried I just can't get past some of the complex math parts of machine learning) but man do I love watching what others do with it.\n",
      "\n",
      "I had a cool translation machine learning concept recently but I couldn't follow through with it for a lack of knowledge but man I really hope this kind of thing just increases and increases in power as they learn more, machine learning is so fascinating!\n",
      "\n",
      "/u/turnip-cake This is all pretty cool isn't it?\n",
      "--\n",
      "Absolutely fascinating topic! Cheers to you all\n",
      "--\n",
      ">  I really hope this kind of thing just increases and increases\n",
      "\n",
      "There are many CV papers focusing on individual aspects, such as face detection, age, sex, body pose, emotion, action, and now lip reading. If we could put all these results together into a single model, maybe that would lead to advances in reinforcement learning and image generation.\n",
      "--\n",
      "That's a good idea. Although I imagine you would have to tell it what accent to use, and the tricky part is that through the training process, all input samples get averaged out. So it would produce mouth movements of an idealized individual that is an average of all samples from the training data.\n",
      "--\n",
      "_______________________________________\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "The video leaves the impression that the best of the best results were collected, which naturally makes me downgrade my estimate of the quality of the average results.  This could be unfair to the actual results.   How does it do on average?\n",
      "_______________________________________\n",
      "Did you miss where they compared their 50% success rate with a human expert's 20% success rate? Or are you asking for something else?\n",
      "--\n",
      "Video.  Video of best results shown in OP, how about video of average, typical results?  How about 1 minute worth?  What does it look like?\n",
      "--\n",
      "*shrug* contact them. research doesn't happen in a bubble. they're real people.\n",
      "--\n",
      "But you couple this 50% with speech-to-text's error rate and...\n",
      "--\n",
      "_______________________________________\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "I immagine you could combine this with a speech-to-text solution and get some pretty nice results.\n",
      "_______________________________________\n",
      "I think it already does the speech-to-text bit. If it actually determined the *audio* produced by the mouth it is watching, that would be even more impressive.\n",
      "--\n",
      "_______________________________________\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "This seems more relevant now\n",
      "\n",
      "https://www.youtube.com/watch?v=DHHbS-H22p4\n",
      "_______________________________________\n",
      "And this https://www.youtube.com/watch?v=1s-PiIbzbhw\n",
      "--\n",
      ">[**2001: A Space Odyssey #5 Movie CLIP - Hal Reads Lips (1968) HD [2:32]**](http://youtu.be/1s-PiIbzbhw)\n",
      "\n",
      "> [*^Movieclips*](https://www.youtube.com/channel/UC3gNmTGu-TTbFPpfSs5kNkg) ^in ^Film ^& ^Animation\n",
      "\n",
      ">*^123,054 ^views ^since ^May ^2011*\n",
      "\n",
      "[^bot ^info](http://www.reddit.com/r/youtubefactsbot/wiki/index)\n",
      "--\n",
      "_______________________________________\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "There's a really cool video of someone lip reading the lips off soilders in an old silent world war 1 video. Really brought a new dimension into things. There's probabily a treasure trove of archival footage like this this technique could be applied to. \n",
      "\n",
      "https://www.youtube.com/watch?v=JVS00zz8yFg&feature=youtu.be&t=209\n",
      "_______________________________________\n",
      "_______________________________________\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Still cant wrap my head around how does it do it\n",
      "_______________________________________\n",
      "I'm guessing it heavily relies on context (same as modern voice recognition) rather than just on the mouth movement. I'd imagine it would perform much worse if they tested it on videos of people saying random words.\n",
      "--\n",
      "yes, that was one of the criticism on the latest similar DeepMind publication: they had over 90% success on the weird dataset that makes no sense at all and no result to show for \"in the wild\".\n",
      "--\n",
      "> latest similar DeepMind publication\n",
      "\n",
      "Bit unfair to call either of them DM publications -- they're mostly done by Oxford PhD students.\n",
      "--\n",
      "sorry I did not mean to put any blame on anyone, I just naively said Deepmind thinking people would remember which publication it was. In fact, it was a video and I didn't see the eventual associated publication.\n",
      "--\n",
      "In the white paper, they mention that context is highly critical otherwise phonemes such as 'p' and 'b' are identical.\n",
      "--\n",
      "Well, that's how humans do it and how it should be done. If you slice up real audio and analyze word segments without context then often noone can tell what was said there because the information simply isn't there. Just as with sight, understanding spoken language also happens mostly in the brain, people pretty much fill in the gaps based on context and their expectations - *literally* hearing what they want to hear.\n",
      "--\n",
      "> understanding spoken language also happens mostly in the brain\n",
      "\n",
      "Where else does it happen?\n",
      "--\n",
      "Right, that wasn't a criticism. For actual real-world use, it should rely on context as well as the visual information. However, I would be very curious to know how accurate this system is when it relies only on mouth movement to discern what words are being said. \n",
      "--\n",
      "Perception is a guided hallucination.\n",
      "--\n",
      "Someone posted the publication: [Here](https://arxiv.org/abs/1611.05358).\n",
      "--\n",
      "machine learning with visual images. \n",
      "--\n",
      "_______________________________________\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Other videos in this thread:\n",
      "\n",
      "[Watch Playlist &#9654;](http://subtletv.com/_r5dgoo6?feature=playlist&nline=1)\n",
      "\n",
      "\tVIDEO|COMMENT\n",
      "\t-|-\n",
      "[Casino Lip Readers scene](https://youtube.com/watch?v=DHHbS-H22p4)|[2](https://reddit.com/r/MachineLearning/comments/5dgoo6/_/da4pbcs?context=10#da4pbcs) - This seems more relevant now   \n",
      "[2001: A Space Odyssey #5 Movie CLIP - Hal Reads Lips (1968) HD](https://youtube.com/watch?v=1s-PiIbzbhw)|[1](https://reddit.com/r/MachineLearning/comments/5dgoo6/_/da4r6m5?context=10#da4r6m5) - And this  \n",
      "[Face2Face: Real-time Face Capture and Reenactment of RGB Videos (CVPR 2016 Oral)](https://youtube.com/watch?v=ohmajJTcpNk)|[1](https://reddit.com/r/MachineLearning/comments/5dgoo6/_/da4y8il?context=10#da4y8il) - I was referring to this \"famous\" video in which they map one actor's expressions onto an unsuspecting target (video). The link to the project page is in the youtube page's description. \n",
      "I'm a bot working hard to help Redditors find related videos to watch.\n",
      "***\n",
      "[Play All](http://subtletv.com/_r5dgoo6?feature=playlist&ftrlnk=1) | [Info](https://np.reddit.com/r/SubtleTV/wiki/mentioned_videos) | Get it on [Chrome](https://chrome.google.com/webstore/detail/mentioned-videos-for-redd/fiimkmdalmgffhibfdjnhljpnigcmohf) / [Firefox](https://addons.mozilla.org/en-US/firefox/addon/mentioned-videos-for-reddit)\n",
      "_______________________________________\n",
      "_______________________________________\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\"X-ray-delta-zero to MC, zero-five-three-three.\n",
      "\n",
      "The computer\n",
      "has just reported another\n",
      "predicted failure off the AAC-unit. As you suggested, we\n",
      "are going to wait and see if it\n",
      "fails, but we are quite sure\n",
      "there is nothing wrong with\n",
      "the unit.\n",
      "\n",
      "If a reasonable waiting period\n",
      "proves us to be correct, we\n",
      "feel now that the computer\n",
      "reliability has been seriously\n",
      "impaired, and presents an\n",
      "unacceptable risk pattern to\n",
      "the mission.\n",
      "\n",
      "We believe, under these\n",
      "circumstances, it would be\n",
      "advisable to disconnect the\n",
      "computer from all ship\n",
      "operations and continue the\n",
      "mission under Earth-based\n",
      "computer control.\n",
      "\n",
      "We think the additional risk caused\n",
      "by the ship-to-earth time lag is\n",
      "preferable to having an unreliable\n",
      "on-board computer.\n",
      "\n",
      "One-zero-five-zero, X-ray-delta-one, transmission concluded.\"\n",
      "_______________________________________\n",
      "_______________________________________\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "this is crazy\n",
      "_______________________________________\n",
      "_______________________________________\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "This is spectacular\n",
      "_______________________________________\n",
      "_______________________________________\n"
     ]
    }
   ],
   "source": [
    "for row in posts[-1]['comments']:\n",
    "    print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    print(row[0])\n",
    "    print('_______________________________________')\n",
    "    for item in row[1:]:\n",
    "        print(item)\n",
    "        print('--')\n",
    "    print('_______________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'comments': [<praw.objects.Comment at 0x7f36d02b0668>,\n",
       "   <praw.objects.MoreComments at 0x7f36d02b0400>,\n",
       "   <praw.objects.Comment at 0x7f36d02b0550>,\n",
       "   <praw.objects.Comment at 0x7f36d02b0320>,\n",
       "   <praw.objects.Comment at 0x7f36d02b06a0>,\n",
       "   <praw.objects.Comment at 0x7f36d0264b38>,\n",
       "   <praw.objects.Comment at 0x7f36d0264710>,\n",
       "   <praw.objects.Comment at 0x7f36d0264400>,\n",
       "   <praw.objects.Comment at 0x7f36d0264358>,\n",
       "   <praw.objects.Comment at 0x7f36d0264208>,\n",
       "   <praw.objects.Comment at 0x7f36d0264f60>,\n",
       "   <praw.objects.Comment at 0x7f36d0264cc0>,\n",
       "   <praw.objects.Comment at 0x7f36d028bb38>,\n",
       "   <praw.objects.Comment at 0x7f36d028bb70>,\n",
       "   <praw.objects.Comment at 0x7f36d028ba90>,\n",
       "   <praw.objects.Comment at 0x7f36d02645f8>,\n",
       "   <praw.objects.Comment at 0x7f36d0264d30>,\n",
       "   <praw.objects.Comment at 0x7f36d0264b00>,\n",
       "   <praw.objects.Comment at 0x7f36d0264c50>,\n",
       "   <praw.objects.Comment at 0x7f36d028bba8>,\n",
       "   <praw.objects.Comment at 0x7f36d028bcf8>,\n",
       "   <praw.objects.Comment at 0x7f36d028be80>,\n",
       "   <praw.objects.Comment at 0x7f36d028b630>,\n",
       "   <praw.objects.Comment at 0x7f36d028b2b0>,\n",
       "   <praw.objects.Comment at 0x7f36d028b518>,\n",
       "   <praw.objects.Comment at 0x7f36d028b470>],\n",
       "  'title': '[Discussion] Join us on /r/LearnMachineLearning!'},\n",
       " {'comments': [<praw.objects.Comment at 0x7f36d026def0>,\n",
       "   <praw.objects.Comment at 0x7f36d026de10>,\n",
       "   <praw.objects.Comment at 0x7f36d026dcc0>,\n",
       "   <praw.objects.Comment at 0x7f36d026da58>],\n",
       "  'title': '[D] Machine Learning - WAYR (What Are You Reading) - Week 13'},\n",
       " {'comments': [<praw.objects.Comment at 0x7f36d0b2eb70>,\n",
       "   <praw.objects.Comment at 0x7f36d0b2e9e8>,\n",
       "   <praw.objects.Comment at 0x7f36d0b2e160>,\n",
       "   <praw.objects.Comment at 0x7f36d0b2ebe0>,\n",
       "   <praw.objects.Comment at 0x7f36d01fd2b0>,\n",
       "   <praw.objects.Comment at 0x7f36d01fd208>,\n",
       "   <praw.objects.Comment at 0x7f36d01fd080>,\n",
       "   <praw.objects.Comment at 0x7f36d0167f60>,\n",
       "   <praw.objects.Comment at 0x7f36d0167f28>,\n",
       "   <praw.objects.Comment at 0x7f36d01fd470>,\n",
       "   <praw.objects.Comment at 0x7f36d01fd390>,\n",
       "   <praw.objects.Comment at 0x7f36d01e9390>,\n",
       "   <praw.objects.Comment at 0x7f36d01e9240>,\n",
       "   <praw.objects.Comment at 0x7f36d01e9208>,\n",
       "   <praw.objects.Comment at 0x7f36d01e90f0>,\n",
       "   <praw.objects.Comment at 0x7f36d01e9048>,\n",
       "   <praw.objects.Comment at 0x7f36d01fde48>,\n",
       "   <praw.objects.Comment at 0x7f36d01fdd68>,\n",
       "   <praw.objects.Comment at 0x7f36d01fdcc0>,\n",
       "   <praw.objects.Comment at 0x7f36d01fda58>,\n",
       "   <praw.objects.Comment at 0x7f36d01fda20>,\n",
       "   <praw.objects.Comment at 0x7f36d01fd908>,\n",
       "   <praw.objects.Comment at 0x7f36d01fd860>,\n",
       "   <praw.objects.Comment at 0x7f36d01fd630>,\n",
       "   <praw.objects.Comment at 0x7f36d01e98d0>,\n",
       "   <praw.objects.Comment at 0x7f36d01e9828>,\n",
       "   <praw.objects.Comment at 0x7f36d01e9780>,\n",
       "   <praw.objects.Comment at 0x7f36d01e9668>,\n",
       "   <praw.objects.Comment at 0x7f36d01e9550>,\n",
       "   <praw.objects.Comment at 0x7f36d01e9b00>,\n",
       "   <praw.objects.Comment at 0x7f36d01e9a20>,\n",
       "   <praw.objects.Comment at 0x7f36d01e9e10>,\n",
       "   <praw.objects.Comment at 0x7f36d01e9d68>,\n",
       "   <praw.objects.Comment at 0x7f36d01e9c88>,\n",
       "   <praw.objects.Comment at 0x7f36d01e9e48>,\n",
       "   <praw.objects.Comment at 0x7f36d0245b38>,\n",
       "   <praw.objects.Comment at 0x7f36d026db38>,\n",
       "   <praw.objects.Comment at 0x7f36d0245a58>,\n",
       "   <praw.objects.Comment at 0x7f36d0245940>,\n",
       "   <praw.objects.Comment at 0x7f36d02457f0>,\n",
       "   <praw.objects.Comment at 0x7f36d02457b8>,\n",
       "   <praw.objects.Comment at 0x7f36d02456d8>,\n",
       "   <praw.objects.Comment at 0x7f36d02454a8>,\n",
       "   <praw.objects.Comment at 0x7f36d02453c8>,\n",
       "   <praw.objects.Comment at 0x7f36d02452e8>,\n",
       "   <praw.objects.Comment at 0x7f36d0245208>,\n",
       "   <praw.objects.Comment at 0x7f36d0245160>,\n",
       "   <praw.objects.Comment at 0x7f36d0245ba8>,\n",
       "   <praw.objects.Comment at 0x7f36d0245d68>,\n",
       "   <praw.objects.Comment at 0x7f36d0245dd8>,\n",
       "   <praw.objects.Comment at 0x7f36d0245eb8>],\n",
       "  'title': '[R] Lip Reading Sentences in the Wild, \"surpasses the performance of all previous work\"'}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Happy (and relieved) to see a more realistic environment without the hard-sell and hype of the previous lip reading paper. Great work! Impossible is nothing ;)'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts[-1]['comments'][3].body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I would make two sweeping suggestions:\\n\\n1. Move your game() into a class. You have a significant amount of state-tracking variables in that function such as score, count, pop, pop2z, and a lot of game logic that could be put into methods.\\n\\n2. Revise some of your classes. You have house, house1, house2 and house3 and they barely differ. Why not have a house class that you implement separate instances of, or at least subclass so you aren't repeating yourself (same goes for zombieupleft and zombieupright). Also, revisit the logic behind the amount of parameters you're passing into a method like:\\n\\n# \\n    def update(self, val, val2, val3, hou1m, h1, h, clicked):\\n        self.rect.x += 1\\n        self.rect.y -= 3\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts[-1][1].body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_posts(name, limit):\n",
    "    subreddit = r.get_subreddit(\"machinelearning\")\n",
    "    posts = []\n",
    "    for post in subreddit.get_hot(limit=limit):\n",
    "        posts.append({'title' : post.title,\n",
    "                     'text' : post.selftext,\n",
    "                     'score' : post.score,\n",
    "                     'url' : post.url,\n",
    "                     'comments' : extract_comments(post.comments)})\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_comments(comments):\n",
    "    return [item.body for item in comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posts = get_posts(name='machinelearning', limit=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[R] Lip Reading Sentences in the Wild, \"surpasses the performance of all previous work\"\n",
      "Is this real?  \n",
      "\n",
      "____________________________\n",
      "\n",
      "Happy (and relieved) to see a more realistic environment without the hard-sell and hype of the previous lip reading paper. Great work! Impossible is nothing ;)\n",
      "\n",
      "____________________________\n",
      "\n",
      "The paper: https://arxiv.org/abs/1611.05358\n",
      "\n",
      "So wait. Oxford and DeepMind released [this one](https://openreview.net/forum?id=BkjLkSqxg) a few weeks ago, and now Oxford and DeepMind release this paper with none of the previous authors? How do they compare \"in the wild\"? Why not collaborate?\n",
      "\n",
      "____________________________\n",
      "\n",
      "Bin blue by m four please.\n",
      "\n",
      "____________________________\n",
      "\n",
      "Dumb question, but could the technique be performed 'in reverse' to produce convincing mouth movements from speech/text?\n",
      "\n",
      "edit: Anyone want to collab on it?\n",
      "\n",
      "____________________________\n",
      "\n",
      "The video leaves the impression that the best of the best results were collected, which naturally makes me downgrade my estimate of the quality of the average results.  This could be unfair to the actual results.   How does it do on average?\n",
      "\n",
      "____________________________\n",
      "\n",
      "I immagine you could combine this with a speech-to-text solution and get some pretty nice results.\n",
      "\n",
      "____________________________\n",
      "\n",
      "This seems more relevant now\n",
      "\n",
      "https://www.youtube.com/watch?v=DHHbS-H22p4\n",
      "\n",
      "____________________________\n",
      "\n",
      "Still cant wrap my head around how does it do it\n",
      "\n",
      "____________________________\n",
      "\n",
      "Other videos in this thread:\n",
      "\n",
      "[Watch Playlist &#9654;](http://subtletv.com/_r5dgoo6?feature=playlist&nline=1)\n",
      "\n",
      "\tVIDEO|COMMENT\n",
      "\t-|-\n",
      "[Casino Lip Readers scene](https://youtube.com/watch?v=DHHbS-H22p4)|[2](https://reddit.com/r/MachineLearning/comments/5dgoo6/_/da4pbcs?context=10#da4pbcs) - This seems more relevant now   \n",
      "[2001: A Space Odyssey #5 Movie CLIP - Hal Reads Lips (1968) HD](https://youtube.com/watch?v=1s-PiIbzbhw)|[1](https://reddit.com/r/MachineLearning/comments/5dgoo6/_/da4r6m5?context=10#da4r6m5) - And this  \n",
      "[Face2Face: Real-time Face Capture and Reenactment of RGB Videos (CVPR 2016 Oral)](https://youtube.com/watch?v=ohmajJTcpNk)|[1](https://reddit.com/r/MachineLearning/comments/5dgoo6/_/da4y8il?context=10#da4y8il) - I was referring to this \"famous\" video in which they map one actor's expressions onto an unsuspecting target (video). The link to the project page is in the youtube page's description. \n",
      "I'm a bot working hard to help Redditors find related videos to watch.\n",
      "***\n",
      "[Play All](http://subtletv.com/_r5dgoo6?feature=playlist&ftrlnk=1) | [Info](https://np.reddit.com/r/SubtleTV/wiki/mentioned_videos) | Get it on [Chrome](https://chrome.google.com/webstore/detail/mentioned-videos-for-redd/fiimkmdalmgffhibfdjnhljpnigcmohf) / [Firefox](https://addons.mozilla.org/en-US/firefox/addon/mentioned-videos-for-reddit)\n",
      "\n",
      "____________________________\n",
      "\n",
      "There's a really cool video of someone lip reading the lips off soilders in an old silent world war 1 video. Really brought a new dimension into things. There's probabily a treasure trove of archival footage like this this technique could be applied to. \n",
      "\n",
      "https://www.youtube.com/watch?v=JVS00zz8yFg&feature=youtu.be&t=209\n",
      "\n",
      "____________________________\n",
      "\n",
      "\"X-ray-delta-zero to MC, zero-five-three-three.\n",
      "\n",
      "The computer\n",
      "has just reported another\n",
      "predicted failure off the AAC-unit. As you suggested, we\n",
      "are going to wait and see if it\n",
      "fails, but we are quite sure\n",
      "there is nothing wrong with\n",
      "the unit.\n",
      "\n",
      "If a reasonable waiting period\n",
      "proves us to be correct, we\n",
      "feel now that the computer\n",
      "reliability has been seriously\n",
      "impaired, and presents an\n",
      "unacceptable risk pattern to\n",
      "the mission.\n",
      "\n",
      "We believe, under these\n",
      "circumstances, it would be\n",
      "advisable to disconnect the\n",
      "computer from all ship\n",
      "operations and continue the\n",
      "mission under Earth-based\n",
      "computer control.\n",
      "\n",
      "We think the additional risk caused\n",
      "by the ship-to-earth time lag is\n",
      "preferable to having an unreliable\n",
      "on-board computer.\n",
      "\n",
      "One-zero-five-zero, X-ray-delta-one, transmission concluded.\"\n",
      "\n",
      "____________________________\n",
      "\n",
      "this is crazy\n",
      "\n",
      "____________________________\n",
      "\n",
      "This is spectacular\n",
      "\n",
      "____________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(posts[-1]['title'])\n",
    "for item in posts[-1]['comments']:\n",
    "    print(item)\n",
    "    print('\\n____________________________\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('ml_test.pkl','wb') as f:\n",
    "    pickle.dump(posts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('ml_test.pkl','rb') as f:\n",
    "    _posts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Discussion] Graph IR for ML frameworks'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_posts[-3]['title']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
